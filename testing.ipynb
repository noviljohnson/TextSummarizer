{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\novil\\anaconda3\\envs\\summy\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"./LaMini-Flan-T5-248M\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint )\n",
    "\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(checkpoint , device_map='auto',\n",
    "                                                        torch_dtype = torch.float32,\n",
    "                                                        offload_folder = './LaMini-Flan-T5-248M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"MBZUAI/LaMini-Flan-T5-248M\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"MBZUAI/LaMini-Flan-T5-248M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"summarization\",\n",
    "    model = base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length = 500,\n",
    "    min_length = 50\n",
    ")\n",
    "def summy(input_text, pipe = pipe):\n",
    "    result = pipe(input_text)\n",
    "    summary = result[0]['summary_text']\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Semantic search seeks to improve search accuracy by understanding the content of the search query instead of relying on lexical matching only. This is done leveraging similarities between embeddings.\n",
    "\n",
    "The idea behind semantic search is to embed all entries in your corpus into a vector space. At search time, the query is embedded into the same vector space and the closest embeddings from your corpus are found.\n",
    "\n",
    "Semantic Search can be performed using the semantic_search function of the util module, which works on the embeddings of the documents in a corpus and on the embeddings of the queries.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 500, but your input_length is only 137. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=68)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Semantic search improves search accuracy by understanding the content of the search query by leveraging similarities between embeddings. It embeds all entries in a corpus into a vector space at search time, and at the same time, the closest embeddas from the corpus are found. The semantic_search function of the util module works on the embedddings of the documents and the queries.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summy(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n'Semantic search improves search accuracy by understanding the content of the search query by leveraging similarities between embeddings. It embeds all entries in a corpus into a vector space at search time, and at the same time, the closest embeddas from the corpus are found. The semantic_search function of the util module works on the embedddings of the documents and the queries.'\\n\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Semantic search improves search accuracy by understanding the content of the search query by leveraging similarities between embeddings. It embeds all entries in a corpus into a vector space at search time, and at the same time, the closest embeddas from the corpus are found. The semantic_search function of the util module works on the embedddings of the documents and the queries.'\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New pipe line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "checkpoint = \"./LaMini-Flan-T5-248M\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint )\n",
    "\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(checkpoint , device_map='auto', torch_dtype = torch.float32, offload_folder=\"offload\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"summarization\",\n",
    "    model = base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length = 500,\n",
    "    min_length = 50\n",
    ")\n",
    "\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template =\"\"\" give a title for the given input text.\n",
    "\n",
    "Input text : {input_text}\n",
    "\n",
    "return the answer in the following format.\n",
    "\n",
    "Answer: \n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | hf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"In this section we compare various aspects of self-attention layers to the recurrent and convolu\u0002tional layers commonly used for mapping one variable-length sequence of symbol representations\n",
    "(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi\n",
    ", zi âˆˆ R\n",
    "d\n",
    ", such as a hidden\n",
    "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
    "consider three desiderata.\n",
    "One is the total computational complexity per layer. Another is the amount of computation that can\n",
    "be parallelized, as measured by the minimum number of sequential operations required.\n",
    "The third is the path length between long-range dependencies in the network. Learning long-range\n",
    "dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\n",
    "ability to learn such dependencies is the length of the paths forward and backward signals have to\n",
    "traverse in the network. The shorter these paths between any combination of positions in the input\n",
    "and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\n",
    "the maximum path length between any two input and output positions in networks composed of the\n",
    "different layer types.\n",
    "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\n",
    "executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\n",
    "computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with\n",
    "sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
    "[38] and byte-pair [31] representations. To improve computational performance for tasks involving\n",
    "very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n",
    "the input sequence centered around the respective output position. This would increase the maximum\n",
    "path length to O(n/r). We plan to investigate this approach further in future work\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 500, but your input_length is only 477. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=238)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Comparing Self-attention Layers to Recurrent and Convolutional Layers for Sequence Transduction Tasks and Learning Long-Range Dependencies in Networks with Xi, Zi  R d and Path Length between Long-Response Signals.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(chain.invoke({\"input_text\": input_text}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The text compares self-attention layers to recurrent and convolutional layers for mapping variable-length sequences of symbol representations and compares the computational complexity per layer, parallelization, and path length between long-range dependencies in the network. The author explains that learning long-rang dependencies is a key challenge in many sequence transduction tasks and considers the length of the paths forward and backward signals to improve computational performance. Self-attention is faster when the sequence length n is smaller than the representation dimensionality d, which is most often used with sentence representations used by state-of-the-art models in machine translations.\n",
    "\n",
    "\n",
    "Title: Comparing Self-attention Layers to Recurrent and Convolutional Layers for Sequence Transduction Tasks and Learning Long-Range Dependencies in Networks with Xi, Zi  R d and Path Length between Long-Response Signals.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid \n",
    "  \n",
    "id = uuid.uuid1(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280304693971884119651480603231246090241"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id.int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110661751864737925840623571635704135056,\n",
       " 21554683291254377879421471393988273732)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uuid.uuid1().int, uuid.uuid4().int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "docs = text_splitter.split_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Semantic search seeks to improve search accuracy by understanding the content of the search query',\n",
       " 'query instead of relying on lexical matching only. This is done leveraging similarities between',\n",
       " 'between embeddings.',\n",
       " 'The idea behind semantic search is to embed all entries in your corpus into a vector space. At',\n",
       " 'space. At search time, the query is embedded into the same vector space and the closest embeddings',\n",
       " 'from your corpus are found.',\n",
       " 'Semantic Search can be performed using the semantic_search function of the util module, which works',\n",
       " 'works on the embeddings of the documents in a corpus and on the embeddings of the queries.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing mysql connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Alice')\n",
      "(2, 'Bob')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating a connection object to a database file\n",
    "conn = sqlite3.connect('example.db')\n",
    "# Creating a cursor object to execute SQL commands\n",
    "cur = conn.cursor()\n",
    "# Creating a table named users with two columns: id and name\n",
    "cur.execute('CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT)')\n",
    "# Inserting some records into the table\n",
    "cur.execute('INSERT INTO users (name) VALUES (\"Alice\")')\n",
    "cur.execute('INSERT INTO users (name) VALUES (\"Bob\")')\n",
    "# Committing the changes to the database\n",
    "conn.commit()\n",
    "# Querying the table and fetching all the records\n",
    "cur.execute('SELECT * FROM users')\n",
    "rows = cur.fetchall()\n",
    "# Printing the records\n",
    "for row in rows:\n",
    "    print(row)\n",
    "# Closing the connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
