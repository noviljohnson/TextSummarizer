{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\novil\\anaconda3\\envs\\summy\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"./LaMini-Flan-T5-248M\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint )\n",
    "\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(checkpoint , device_map='auto',\n",
    "                                                        torch_dtype = torch.float32,\n",
    "                                                        offload_folder = './LaMini-Flan-T5-248M')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"MBZUAI/LaMini-Flan-T5-248M\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"MBZUAI/LaMini-Flan-T5-248M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"summarization\",\n",
    "    model = base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length = 500,\n",
    "    min_length = 50\n",
    ")\n",
    "def summy(input_text, pipe = pipe):\n",
    "    result = pipe(input_text)\n",
    "    summary = result[0]['summary_text']\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Semantic search seeks to improve search accuracy by understanding the content of the search query instead of relying on lexical matching only. This is done leveraging similarities between embeddings.\n",
    "\n",
    "The idea behind semantic search is to embed all entries in your corpus into a vector space. At search time, the query is embedded into the same vector space and the closest embeddings from your corpus are found.\n",
    "\n",
    "Semantic Search can be performed using the semantic_search function of the util module, which works on the embeddings of the documents in a corpus and on the embeddings of the queries.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 500, but your input_length is only 137. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=68)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Semantic search improves search accuracy by understanding the content of the search query by leveraging similarities between embeddings. It embeds all entries in a corpus into a vector space at search time, and at the same time, the closest embeddas from the corpus are found. The semantic_search function of the util module works on the embedddings of the documents and the queries.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summy(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n'Semantic search improves search accuracy by understanding the content of the search query by leveraging similarities between embeddings. It embeds all entries in a corpus into a vector space at search time, and at the same time, the closest embeddas from the corpus are found. The semantic_search function of the util module works on the embedddings of the documents and the queries.'\\n\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "'Semantic search improves search accuracy by understanding the content of the search query by leveraging similarities between embeddings. It embeds all entries in a corpus into a vector space at search time, and at the same time, the closest embeddas from the corpus are found. The semantic_search function of the util module works on the embedddings of the documents and the queries.'\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New pipe line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "\n",
    "checkpoint = \"./LaMini-Flan-T5-248M\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint )\n",
    "\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(checkpoint , device_map='auto', torch_dtype = torch.float32, offload_folder=\"offload\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"summarization\",\n",
    "    model = base_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length = 500,\n",
    "    min_length = 50\n",
    ")\n",
    "\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template =\"\"\" give a title for the given input text.\n",
    "\n",
    "Input text : {input_text}\n",
    "\n",
    "return the answer in the following format.\n",
    "\n",
    "Answer: \n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | hf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"In this section we compare various aspects of self-attention layers to the recurrent and convolu\u0002tional layers commonly used for mapping one variable-length sequence of symbol representations\n",
    "(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi\n",
    ", zi âˆˆ R\n",
    "d\n",
    ", such as a hidden\n",
    "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
    "consider three desiderata.\n",
    "One is the total computational complexity per layer. Another is the amount of computation that can\n",
    "be parallelized, as measured by the minimum number of sequential operations required.\n",
    "The third is the path length between long-range dependencies in the network. Learning long-range\n",
    "dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\n",
    "ability to learn such dependencies is the length of the paths forward and backward signals have to\n",
    "traverse in the network. The shorter these paths between any combination of positions in the input\n",
    "and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\n",
    "the maximum path length between any two input and output positions in networks composed of the\n",
    "different layer types.\n",
    "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\n",
    "executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\n",
    "computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with\n",
    "sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
    "[38] and byte-pair [31] representations. To improve computational performance for tasks involving\n",
    "very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n",
    "the input sequence centered around the respective output position. This would increase the maximum\n",
    "path length to O(n/r). We plan to investigate this approach further in future work\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 500, but your input_length is only 477. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=238)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Comparing Self-attention Layers to Recurrent and Convolutional Layers for Sequence Transduction Tasks and Learning Long-Range Dependencies in Networks with Xi, Zi  R d and Path Length between Long-Response Signals.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(chain.invoke({\"input_text\": input_text}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The text compares self-attention layers to recurrent and convolutional layers for mapping variable-length sequences of symbol representations and compares the computational complexity per layer, parallelization, and path length between long-range dependencies in the network. The author explains that learning long-rang dependencies is a key challenge in many sequence transduction tasks and considers the length of the paths forward and backward signals to improve computational performance. Self-attention is faster when the sequence length n is smaller than the representation dimensionality d, which is most often used with sentence representations used by state-of-the-art models in machine translations.\n",
    "\n",
    "\n",
    "Title: Comparing Self-attention Layers to Recurrent and Convolutional Layers for Sequence Transduction Tasks and Learning Long-Range Dependencies in Networks with Xi, Zi  R d and Path Length between Long-Response Signals.\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid \n",
    "  \n",
    "id = uuid.uuid1(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280304693971884119651480603231246090241"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id.int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110661751864737925840623571635704135056,\n",
       " 21554683291254377879421471393988273732)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uuid.uuid1().int, uuid.uuid4().int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "docs = text_splitter.split_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Semantic search seeks to improve search accuracy by understanding the content of the search query',\n",
       " 'query instead of relying on lexical matching only. This is done leveraging similarities between',\n",
       " 'between embeddings.',\n",
       " 'The idea behind semantic search is to embed all entries in your corpus into a vector space. At',\n",
       " 'space. At search time, the query is embedded into the same vector space and the closest embeddings',\n",
       " 'from your corpus are found.',\n",
       " 'Semantic Search can be performed using the semantic_search function of the util module, which works',\n",
       " 'works on the embeddings of the documents in a corpus and on the embeddings of the queries.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing mysql connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Alice')\n",
      "(2, 'Bob')\n",
      "(3, 'Don')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating a connection object to a database file\n",
    "conn = sqlite3.connect('example.db')\n",
    "# Creating a cursor object to execute SQL commands\n",
    "cur = conn.cursor()\n",
    "# Creating a table named users with two columns: id and name\n",
    "cur.execute('CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT)')\n",
    "# Inserting some records into the table\n",
    "cur.execute('INSERT INTO users (name) VALUES (\"Alice\")')\n",
    "cur.execute('INSERT INTO users (name) VALUES (\"Bob\")')\n",
    "cur.execute('INSERT INTO users (name) VALUES (\"Don\")')\n",
    "# Committing the changes to the database\n",
    "conn.commit()\n",
    "# Querying the table and fetching all the records\n",
    "cur.execute('SELECT * FROM users')\n",
    "rows = cur.fetchall()\n",
    "# Printing the records\n",
    "for row in rows:\n",
    "    print(row)\n",
    "# Closing the connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect('example.db')\n",
    "# Creating a cursor object to execute SQL commands\n",
    "cur = conn.cursor()\n",
    "name = \"a@#b\"\n",
    "cur.execute(f'''Select * from users where name = \"{name}\"''')\n",
    "records = cur.fetchall()\n",
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Select * from users where name = \"a@#b\"'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'Select * from users where name = \"{name}\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QA pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\novil\\anaconda3\\envs\\summy\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If the answer can be inferred from the context, provide it with proper punctuation and capitalization. \n",
    "# If the answer is not present in the context but can be addressed with your model knowledge up to your last update, use that information to answer. \n",
    "# If you still don't know the answer, state that you don't know. Do not fabricate an answer.\n",
    "\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "# Generate point-wise answers for the question if required, and format them clearly with bullet points or numbers.\n",
    "# Conclude or explain the context based on the question's intent when possible.\n",
    "# If the answer is not present in the context, attempt to provide a close enough answer using the model's knowledge without making assumptions beyond that knowledge.\n",
    "# Answer:\"\"\"\n",
    "# PROMPT = PromptTemplate(\n",
    "#     template=prompt_template, input_variables=[\"context\", \"question\"]#, \"sources\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model name you want to use\n",
    "model_name = \"Intel/dynamic_tinybert\"\n",
    "# model_name = \"./LaMini-Flan-T5-248M\"\n",
    "\n",
    "# Load the tokenizer associated with the specified model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Define a question-answering pipeline using the model and tokenizer\n",
    "question_answerer = pipeline(\n",
    "    \"question-answering\", \n",
    "    model=model_name, \n",
    "    tokenizer=tokenizer,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
    "# with additional model-specific arguments (temperature and max_length)\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=question_answerer,\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever object from the 'db' with a search configuration where it retrieves up to 4 relevant splits/documents.\n",
    "# retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# # Create a question-answering instance (qa) using the RetrievalQA class.\n",
    "# # It's configured with a language model (llm), a chain type \"refine,\" the retriever we created, and an option to not return source documents.\n",
    "# qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"refine\", retriever=retriever, return_source_documents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations\n",
    "(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi\n",
    ", zi âˆˆ R\n",
    "d\n",
    ", such as a hidden\n",
    "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\n",
    "consider three desiderata.\n",
    "One is the total computational complexity per layer. Another is the amount of computation that can\n",
    "be parallelized, as measured by the minimum number of sequential operations required.\n",
    "The third is the path length between long-range dependencies in the network. Learning long-range\n",
    "dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\n",
    "ability to learn such dependencies is the length of the paths forward and backward signals have to\n",
    "traverse in the network. The shorter these paths between any combination of positions in the input\n",
    "and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\n",
    "the maximum path length between any two input and output positions in networks composed of the\n",
    "different layer types.\n",
    "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\n",
    "executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\n",
    "computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with\n",
    "sentence representations used by state-of-the-art models in machine translations, such as word-piece\n",
    "[38] and byte-pair [31] representations. To improve computational performance for tasks involving\n",
    "very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\n",
    "the input sequence centered around the respective output position. This would increase the maximum\n",
    "path length to O(n/r). We plan to investigate this approach further in future work\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "ld = Document(page_content=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 11\u001b[0m\n\u001b[0;32m      6\u001b[0m PROMPT \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[0;32m      7\u001b[0m     template\u001b[38;5;241m=\u001b[39mtemplate, input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;66;03m#, \"sources\"]\u001b[39;00m\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m chain\u001b[38;5;241m=\u001b[39mload_qa_chain(llm,  chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m, prompt\u001b[38;5;241m=\u001b[39mPROMPT)\n\u001b[1;32m---> 11\u001b[0m res\u001b[38;5;241m=\u001b[39m\u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_documents\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mld\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\novil\\anaconda3\\envs\\summy\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\novil\\anaconda3\\envs\\summy\\lib\\site-packages\\langchain\\chains\\base.py:378\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    371\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    376\u001b[0m }\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\novil\\anaconda3\\envs\\summy\\lib\\site-packages\\langchain\\chains\\base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\novil\\anaconda3\\envs\\summy\\lib\\site-packages\\langchain\\chains\\base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    160\u001b[0m     )\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\novil\\anaconda3\\envs\\summy\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:137\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m    136\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[1;32m--> 137\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_docs(\n\u001b[0;32m    138\u001b[0m     docs, callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mother_keys\n\u001b[0;32m    139\u001b[0m )\n\u001b[0;32m    140\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32mc:\\Users\\novil\\anaconda3\\envs\\summy\\lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py:242\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[1;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombine_docs\u001b[39m(\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m, docs: List[Document], callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m    230\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[0;32m    231\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Stuff all documents into one prompt and pass to LLM.\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03m        element returned is a dictionary of other keys to return.\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 242\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs), {}\n",
      "File \u001b[1;32mc:\\Users\\novil\\anaconda3\\envs\\summy\\lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py:198\u001b[0m, in \u001b[0;36mStuffDocumentsChain._get_inputs\u001b[1;34m(self, docs, **kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct inputs from kwargs and docs.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03mFormat and then join all the documents together into one input with name\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m    dictionary of inputs to LLMChain\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# Format each document according to the prompt\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m doc_strings \u001b[38;5;241m=\u001b[39m [format_document(doc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocument_prompt) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Join the documents together to put them in the prompt.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    201\u001b[0m     k: v\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39minput_variables\n\u001b[0;32m    204\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\novil\\anaconda3\\envs\\summy\\lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py:198\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct inputs from kwargs and docs.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03mFormat and then join all the documents together into one input with name\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m    dictionary of inputs to LLMChain\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# Format each document according to the prompt\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m doc_strings \u001b[38;5;241m=\u001b[39m [\u001b[43mformat_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument_prompt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Join the documents together to put them in the prompt.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    201\u001b[0m     k: v\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39minput_variables\n\u001b[0;32m    204\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\novil\\anaconda3\\envs\\summy\\lib\\site-packages\\langchain_core\\prompts\\base.py:332\u001b[0m, in \u001b[0;36mformat_document\u001b[1;34m(doc, prompt)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_document\u001b[39m(doc: Document, prompt: BasePromptTemplate[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    299\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format a document into a string based on a prompt template.\u001b[39;00m\n\u001b[0;32m    300\u001b[0m \n\u001b[0;32m    301\u001b[0m \u001b[38;5;124;03m    First, this pulls information from the document from two sources:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;124;03m            >>> \"Page 1: This is a joke\"\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prompt\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[43m_get_document_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\novil\\anaconda3\\envs\\summy\\lib\\site-packages\\langchain_core\\prompts\\base.py:284\u001b[0m, in \u001b[0;36m_get_document_info\u001b[1;34m(doc, prompt)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_document_info\u001b[39m(doc: Document, prompt: BasePromptTemplate[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[1;32m--> 284\u001b[0m     base_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_content\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdoc\u001b[38;5;241m.\u001b[39mmetadata}\n\u001b[0;32m    285\u001b[0m     missing_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(prompt\u001b[38;5;241m.\u001b[39minput_variables)\u001b[38;5;241m.\u001b[39mdifference(base_info)\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_metadata) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "# llm_model = OpenAIChat(temperature=0.25,model_name=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "template = \"\"\" summarize the given text \n",
    "text : {context}\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=template, input_variables=[\"context\"]#, \"sources\"]\n",
    ")\n",
    "\n",
    "chain=load_qa_chain(llm,  chain_type=\"stuff\", prompt=PROMPT)\n",
    "res=chain({\"input_documents\": ld, \"context\": input_text })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### API testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \" http://127.0.0.1:5000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = 'novil@gmail.com'\n",
    "pwd = 'N0M@1001'\n",
    "res = requests.get(url+'/login', json={'email':email, 'password':pwd})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log': 'No Details Found'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_name = 'Novil sai'\n",
    "\n",
    "\n",
    "res = requests.get(url+'/update_user', json={'email':email, 'password':pwd, \"full_name\":full_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'log': 'Exists'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"RAG, or retrieval-augmented generation, is a new way to understand and create language. It combines two kinds of models. First, retrieve relevant information. Second, generate text from that information. By using both together, RAG does an amazing job. Each modelâ€™s strengths make up for the otherâ€™s weaknesses. So RAG stands out as a groundbreaking method in natural language processing.\n",
    "RAG combines retrieval models that extract data from vast knowledge repositories with generative models that formulate pertinent responses. RAG produces responses rooted in factual information.\n",
    "The interplay between these components enables RAG to be more informative and accurate than conventional generational models operating independently.\n",
    "RAG has the skills to comprehend and answer hard questions. How does it work? It uses knowledge from searching to understand the content better, so its outputs are more relevant and informative.\n",
    "RAG adapts well, easily working with different NLP apps like answering questions, dialogue systems, and making content. Its flexibility allows developers to use its abilities across many natural language understanding and generation tasks without much trouble.\n",
    "Improved Accuracy: RAG combines the benefits of retrieval-based and generative models, leading to more accurate and contextually relevant responses.\n",
    "Enhanced Contextual Understanding: By retrieving and incorporating relevant knowledge from a knowledge base, RAG demonstrates a deeper understanding of queries, resulting in more precise answers.\n",
    "Reduced Bias and Misinformation: RAGâ€™s reliance on verified knowledge sources helps mitigate bias and reduces the spread of misinformation compared to purely generative models.\n",
    "Versatility: RAG can be applied to various natural language processing tasks, such as question answering, chatbots, and content generation, making it a versatile tool for language-related applications.\n",
    "Empowering Human-AI Collaboration: RAG can assist humans by providing valuable insights and information, enhancing collaboration between humans and AI systems.\n",
    "Advancement in AI Research: RAG represents a significant advancement in AI research by combining retrieval and generation techniques, pushing the boundaries of natural language understanding and generation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url+\"/summarize\", json={'text':text, \"email\":'novil@gmail.com'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'RAG is a new method in natural language processing that combines two types of models: retrieval and generation. It combines retrieval models that extract relevant information and generative models that formulate pertinent responses. RAG has the skills to comprehend and answer hard questions. It works by using knowledge from searching to understand the content better, resulting in more relevant and informative outputs. It adapts well to different NLP apps and can be used across many natural language understanding and generation tasks without much trouble. Its reliance on verified knowledge sources helps mitigate bias and reduces the spread of misinformation compared to purely generative model. It can assist humans by providing valuable insights and information, enhancing collaboration between humans and AI systems. Advancement in AI research.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"{'summary': 'RAG is a new method in natural language processing that combines two types of models: retrieval and generation. It combines retrieval models that extract relevant information and generative models that formulate pertinent responses. RAG has the skills to comprehend and answer hard questions. It works by using knowledge from searching to understand the content better, resulting in more relevant and informative outputs. It adapts well to different NLP apps and can be used across many natural language understanding and generation tasks without much trouble. Its reliance on verified knowledge sources helps mitigate bias and reduces the spread of misinformation compared to purely generative model. It can assist humans by providing valuable insights and information, enhancing collaboration between humans and AI systems. Advancement in AI research.'}\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
